{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "혀누를 위한 딥러닝",
      "provenance": [],
      "collapsed_sections": [
        "t88oG-qc-16q",
        "qO3dY4c56Gfw",
        "bJWmUSDlAyOK",
        "cu8qZChTmcZK",
        "4uMGUztimedg",
        "lZ6HbDW9C905",
        "FvfPNPNXFrE-",
        "iOHgaMFVFspK",
        "irdcEEPdGjHH",
        "3ZF7qHzDGuTd",
        "jOdk6QUccAwv",
        "nMtUYXOThd3v",
        "_bCY9Vv-jPWc",
        "UfvhxoLWmVPV",
        "V2TFdjGnnOdO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYso35hnbpA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# ┗(•̀へ •́ )ﾉ Hyun's Deep Learning\n",
        "---\n",
        "자 Deep Learning을 공부해봅시다 (´∇｀)\n",
        "![미니언즈!](https://post-phinf.pstatic.net/MjAxNzA3MTBfMjIg/MDAxNDk5NjcxOTY1NDQw.Kz07JXiZg6AT6Y4PAZY7ubUNAr7rbDinLwFGuS0OOxcg.WVhpo8yfybUh0qImMGNAo1ucSUPuNOvQyzlO_vKlAlkg.JPEG/%EC%98%81%EC%A7%84%EC%9C%845.jpg?type=w1200)\n",
        "예이~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t88oG-qc-16q",
        "colab_type": "text"
      },
      "source": [
        "# 목차"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhANASX2-28p",
        "colab_type": "text"
      },
      "source": [
        "## 0. 딥러닝이 무엇일까?\n",
        "## 0. 딥러닝 관련 용어정리 (알파벳 순)\n",
        "## 0. DL 곁눈질하기 with.DNN\n",
        "## 0. CNN\n",
        "## 0. RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3dY4c56Gfw",
        "colab_type": "text"
      },
      "source": [
        "# 0. 딥러닝이 무엇일까???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IknJz98B6eFD",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.filepicker.io/api/file/S4choBT5RyPBKeLqlWTG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2wWk6km_CWp",
        "colab_type": "text"
      },
      "source": [
        "- 머신러닝(이하 ML)의 일부로 ML과 다르게 스스로 학습이 가능하다는 장점이 있다!\n",
        "- 크게 Deep Neural Network(DNN), Convolutional Neural Network(CNN), Recurrent Neural Network(RNN), Generative Adversarial Network(GAN) 등이 있다.\n",
        "- 간단하게 정리하면\n",
        "  1. DNN: Aritifical Nerual Network에서 hidden layer가 2개 이상인 모델\n",
        "  2. CNN: Convolution(합성곱) 연산과 Filter를 활용하여 이미지의 특징을 추출해내는 모델\n",
        "  3. RNN: 순서가 있는 데이터(문장, 주식종가, 음성인식 등)에서 특징을 추출해내는 모델\n",
        "  4. GAN: 가짜의 이미지를 만들어내는 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJWmUSDlAyOK",
        "colab_type": "text"
      },
      "source": [
        "# 0. 딥러닝 관련 용어, 메소드 모음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8qZChTmcZK",
        "colab_type": "text"
      },
      "source": [
        "## ML/DL 용어모음\n",
        " ```용어참조``` 부분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxHP4wuyA0WY",
        "colab_type": "text"
      },
      "source": [
        "> Activation function\n",
        "- 신경망의 출력을 결정하는 식\n",
        "![Activation function](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png) \n",
        "- Non-linear Activation Function을 쓰는 이유는, 활성화 함수가 선형일 경우 Perceptron이 여러 개라도, 즉, Layer을 깊게 쌓더라도 결국 선형 변환이기 때문.\n",
        "- Act_fun을 쓰는 이유\n",
        "  - input 사이의 non-linear 관계를 잡는다\n",
        "  - input을 유용한 output으로 바꿔준다.\n",
        "\n",
        "> Adam optimizer\n",
        "- 기울기의 일차 모멘트(평균), 이차 모멘트(분산)을 계산하여 변수를 업데이트하는 방식\n",
        "\n",
        "> Batch Normalization\n",
        "- Internal Covariate Shift(내부 공변량 변화.하나의 hidden layer에 여러 범위의 입력이 들어오는 현상)를 해결하기 위해 사용\n",
        "- 개념: 각 layer마다 normalization하는 layer을 둬서 변형된 분포가 나오지 않도록 방지하는 것. Batch마다 normalization을 해주겠다는 것\n",
        "- Overfitting을 방지하는 용도로 사용됨\n",
        "- DenseNet 구조: batch normalization -> activation function -> convolution\n",
        "- hidden layer마다 batch normalization을 넣어주면 internal cavariate shift를 방지하면서 더 큰 lr의 사용도 가능해짐.\n",
        "- batch normalization은 학습 시 배치 단위의 평균, 분산을 받아 이동평균, 이동분산을 저장한 뒤 test할 때 구해놓은 평균, 분산으로 정규화를 한다.\n",
        "\n",
        "> Batch size\n",
        "- Batch size: data를 나누는 것\n",
        "\n",
        "> Backpropagation(역전파)  \n",
        "[김성훈 교수님의 강의](https://www.youtube.com/watch?v=573EZkzfnZ0&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=27)\n",
        "- loss를 각 변수들로 미분하여 기울기를 구하고 chain rule을 이용하여 입력 단까지 다시 전달하는 과정\n",
        "- 입력 X가 들어왔을 때, Neural Network를 통해서 output Y를 구하는데 loss를 계산해서 weight의 미분값을 계산해서 loss를 최소화하도록 weight를 업데이트하는 방식\n",
        "- 자유투를 던지는 과정은 순전파 과정(Feed Forward)라고 할 수 있고, 던진 공이 어느 지점에 도착했는지를 확인하고 던질 위치를 수정하는 과정이 역전파(Backpropagation)이다.\n",
        "- 내 생각: loss에 각 weight별로 어느 정도 영향을 미쳤는지를 고려했는지 알기 위해서 사용하는 방법\n",
        "\n",
        "> Binary cross entropy\n",
        "- $BCE(x) = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log\\big(h(x_i; \\theta)\\big) + (1-y_i) \\log\\big(1- h(x_i; \\theta)\\big)$\n",
        "- 두 개의 class 중 하나를 예측하는 task에 대한 cross entropy의 special case\n",
        "\n",
        "> BPTT(Backpropagation throught time)\n",
        " - loss를 input과 hidden layer 사이의 gradient로 미분하여 loss에 대한 각각의 비중을 구해 업데이트하면 됨.\n",
        "  \n",
        "> CBOW(continuous bag-of-words)\n",
        "- embedding 방식 중 하나\n",
        "- 주변 단어들로부터 가운데 들어갈 단어가 나오도록 embedding하는 방식\n",
        "- 이렇게 하면 주위 단어들과 특정 단어와의 관계가 학습이 되고, 문장 또는 문서의 모든 문장을 위와 같은 방식으로 학습하면 그 문서에서 사용한 단어들이 의미적(semantcally)으로 embedding된다.\n",
        "\n",
        "> Convolution\n",
        "- 이미지 위에서 stride 값 만큼 filter(kernel)을 이동시키면서 겹쳐지는 부분의 각 원소의 값을 곱해서 모두 더한 값을 출력하는 연산\n",
        "  - stride: filter가 한번에 얼마나 이동할 것인가\n",
        "  - padding: 일정한 크기의 층으로 이미지를 감싸는 것. 입력 이미지에서 충분한 특성을 뽑을 수 있기 때문에 padding을 사용!!!\n",
        "- input type: Tensor형태이어야 함\n",
        "- input shape: (N x C x H x W) = (batch_size, channel, height, width)\n",
        "- $$Output size = \\frac{input size- filter size + (2 * padding)}{Stride} + 1$$\n",
        "\n",
        "> Cost function\n",
        "- Cost function은 loss function의 합이다. 즉 entire data set을 다루는 것이다.\n",
        "- 순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n",
        "- A loss function **is a part of** a cost function **which is a type of** an objective function!!!\n",
        "\n",
        "> Cross-entropy  \n",
        "- $H_{p,q}(X) = - \\sum_{i=1}^N p(x_i) \\log q(x_i)$\n",
        "- 목표로 하는 최적의 확률분포 p와 이를 근사하려는 확률분포 q가 얼마나 다른지 측정하는 방법. 즉 원래 p였던 분포를 q로 표현했을 때 얼만큼의 비용이 드는지 측정하는 것\n",
        "- p는 true probability, true label에 대한 분, q는 현재 예측모델의 추정값에 대한 분포\n",
        "- Sigmoid 활성함수를 이용한 이진분류의 cost 함수를 MSE로 정의할 때보다 예측이 잘못될수록 cost가 더 크게 증가하여 학습 면에서도 cross entropy를 사용\n",
        "\n",
        "> Cross-entropy loss\n",
        "- categorical output 예측에 많이 쓰임\n",
        "\n",
        "> Cosine similarity\n",
        "- 의미적 연산의 대표적인 지표\n",
        "- 두 vector의 inner product으로 계산.\n",
        "- one-hot vector는 inner product이 0이 나와서 cosine similarity를 구할 수 없음\n",
        "- 이를 해결하기 위한 것이 embedding\n",
        "\n",
        "> cuDNN\n",
        "- CUDA를 이용해 딥러닝 연산을 가속해주는 라이브러리\n",
        "\n",
        "> CUDA\n",
        "- 엔비디아가 GPU를 통한 연산을 가능하게 만든 API 모델\n",
        "\n",
        "> Data augmentation\n",
        "- 데이터를 늘리는 방법으로 이미지에서 많이 사용\n",
        "- 간단하게 말하면 이미지를 돌리거나 뒤집기만 해도 데이터의 수를 늘리는 효과를 가져옴\n",
        "\n",
        "> Dropout\n",
        "- overfitting을 방지하는 방법 중 하나 (e.g. train data 늘리기, feature 줄이기, regularization)\n",
        "- 학습을 진행하면서 node, 뉴런을 무작위로 껐다 켰다 하는 것.모델의 수용력을 줄임으로써 overfitting을 방지\n",
        "- Network ensemble 효과를 얻을 수도 있음 -> 학습 때 생기는 낮은 수용력의 모델들이 test할 때는 드롭을 하지 않고 합쳐지기 때문 -> 차이를 맞춰주기 위해서 test 시에는 drop 확률을 곱해준다.\n",
        "\n",
        "> e (자연상수, exponential)\n",
        "- $\\lim_{n \\to \\infty} (1+\\frac{1}{n})^n$\n",
        "- 2.718281828459046..\n",
        "- \"성장\". 자연의 연속한 성장을 표현하기 위해 만든 수\n",
        "- 100%의 성장률을 가지고 1회 연속성장 할 때, 가질 수 있는 최대 성장량을 의미\n",
        "- 마법의 돼지 저금통을 생각!!\n",
        "- 예시) 전세계 인구가 x% 성장한다면 1년 뒤에 인구가 몇 명이 될지 e를 이용하면 정확하게 유추가 가능. 이 외 많은 부분에서 자연의 연속성장을 예측할 때 e를 쓸 수 있다.\n",
        "\n",
        "> Embedding\n",
        "- One-hot vector의 의미적 연산과 확장성의 한계(inner product이 항상 0이 나오기에 단어, 문장 간의 의미적 차이나 유사도를 구하는 것이 불가능)를 극복하기 위한 방법\n",
        "- 간단하게 말하면 단어, 알파벳 같은 기본 단위 요소들을 일정한 길이를 가지는 vector 공간에 투영하는 것\n",
        "- 단어를 벡터화하는 것이 word2vec\n",
        "- 대표적으로 CBOW, skip-gram\n",
        "\n",
        "> Entropy\n",
        "- 불확실성에 대한 척도.\n",
        "- Entropy 함수  \n",
        "  $H_p(X) = \\mathbb{E}\\big[I(X)\\big] = \\mathbb{E} \\big[ \\log (\\frac{1}{p(X)}) \\big] = -\\sum_{i=1}^{n} p(x_i)\\log(p(x_i))$  \n",
        "  C는 범주의 갯수, q는 사건의 확률질량함수(probability mass function)\n",
        "- 예측이 어려울수록 정보의 양은 더 많아지고 엔트로피는 커진다.\n",
        "- 확률적으로 발생하는 사건에 대한 **정보량의 평균**. 놀람의 정도를 나타낸다고 볼 수 있다.\n",
        "\n",
        "> Epoch\n",
        "- one epoch: 모든 training example을 도는 것!\n",
        "\n",
        "> Forward propagation\n",
        "- Neural Network에 입력값이 들어와 여러 개의 hidden layer을 순서대로 거쳐 결괏값을 내는 과정\n",
        "- $y = w_4(f(w_3(f(w_2(f(w_1*x+b_1))+b_2))+b_3))+b_4$  \n",
        "  이 과정이 forward propagation\n",
        "\n",
        "> Fully connected layer\n",
        "- 이전 layer의 모든 node가 다음 layer 모든 node에 연결된 layer를 fully connected layer(FC layer)라고 함.\n",
        "\n",
        "> Gradient\n",
        "- Parameter들의 편미분계수, 기울기\n",
        "\n",
        "> Gradient descent\n",
        "- $W_{t+1} = W_t - lr*\\frac{\\delta loss}{\\delta w}$\n",
        "- loss에서 weight가 차지하는 비중만큼 기존 weight를 바꿔주는 방법\n",
        "- $W_{t+1} = W_t - lr*\\frac{\\delta loss}{\\delta w}-lr*\\lambda*w$\n",
        "- gradient descent에 가중치 부식을 넣은 것\n",
        "\n",
        "> Gradient exploding\n",
        "- gradient vanishing과 반대\n",
        "- exploding은 gradient가 너무 크게 계산되어서 발생하는 문제\n",
        "- Solution\n",
        "  - Batch normalization\n",
        "  - Chaning activation function\n",
        "  - careful initialization\n",
        "  - small learning rate\n",
        "\n",
        "> Gram matrix\n",
        "- Gram matrix란 내적이 정의된 공간에서 벡터 $v_1$, $v_2$, ..., $v_n$이 있을 때 가능한 모든 경우의 내적을 행렬로 나타낸 것.\n",
        "- 벡타 하나하나 간의 내적으로 계산하지 않고 행렬곱으로 계산하면 한 번에 Gram matrix를 구할 수 있다.\n",
        "\n",
        "> Hidden state\n",
        "- RNN에서 출력되지 않고 다음 cell로 전달되는 값\n",
        "\n",
        "> Hyperbolic Tangent\n",
        "- $tanh(x) = \\frac{e^x-e^-x}{e^x+e^-x}$\n",
        "- -1 ~ +1 값을 가짐\n",
        "\n",
        "> Hyperparameter\n",
        "- 학습의 대상이 아니라 학습 이전에 정해놓은 변수\n",
        "\n",
        "> Initialization\n",
        "- 최적의 지점 자체에서 시작한다는 말이 성립할 수 없는 대신 모델이 학습되는 도중에 gradient vanishing / exploding 현상을 최소한 겪지 않게 하거나 loss 함수 공간을 최적화가 쉬운 형태로 바꾸는 방법\n",
        "- weight를 초기화해야 학습이 잘 되고 성능이 좋아진다.\n",
        "- RBM, Xavier, He initialization 등등을 사용한다.  \n",
        "  1) Xavier\n",
        "    - 핵심: 가중치의 초기값을 $N(0,var=2/(n_{in}+n_{out}))$에서 뽑는다. \n",
        "    - $n_{in}$: 해당 레이어에 들어오는 특성의 수 \n",
        "    - $n_{out}$: 해당 레이어에 나가는 특성의 수\n",
        "    - 이를 통해 데이터가 몇 개의 layer를 통과하더라도 활성화 값이 너무 커지거나 작아지지 않고 일정한 범위 안에 있도록 잡아주는 초기화 방법\n",
        "    - sigmoid, tanh 함수를 주로 사용하는 경우에 사용하는 initialization  \n",
        "  2) He \n",
        "    - 가중치를 $N(0,var=\\frac{2}{(1+a^2)*n_{in}})$에서 샘플링함\n",
        "    - a: ReLU, Leaky Rulu의 음수 부분의 기울기에 해당\n",
        "    - ReLU를 사용하는 경우에 사용하는 initialization\n",
        "\n",
        "> Internal Covariate Shift\n",
        "- Train set과 Test set의 분포에 차이가 있어서 문제를 발생시킨다는 개념\n",
        "- 이 문제를 해결하기 위해 batch normalization을 사용\n",
        "\n",
        "> Iteration\n",
        "- batch를 몇 번 학습에 사용했냐?\n",
        "- e.g. 1000개의 training set이 있고, batch size는 500이다. 그러므로 1 epoch 도는 동안 2 iteration이다.\n",
        "\n",
        "> KL(Kullback-Leibler) Divergence\n",
        "- $H(p,q) = H(p) + \\sum_x{p(x)log\\frac{p(x)}{q(x)}}$\n",
        "- 분포 p를 기준으로 q가 얼마나 다른지 측정하는 방법\n",
        "- cross entropy를 최소화한다는 것은 KLD를 최소화하여 q가 p의 분포와 최대한 같아지게 한다는 것.\n",
        "\n",
        "> Latent space interpolation (잠재 공간 보간)\n",
        "- 잠재 변수 z의 공간을 탐색하는 방식\n",
        "\n",
        "> L-BFGS(limited-memory BFGS)\n",
        "- style transfer를 구현할 때 쓰는 optimizer 알고리즘으로 2차 미분 값까지 이용한다는 특징이 있음.\n",
        "- 연산 속도는 1차 미분보다 느리지만 엄밀하게 계산하는 대신 근사하는 방식으로 바꿔서 연산 속도를 높이는 장점이 있음.\n",
        "\n",
        "> Leaky RELU\n",
        "- $f(x) = max(ax,x)$\n",
        "- dying neuron 현상을 해결해줌\n",
        "\n",
        "> Learning rate\n",
        "- 계산한 기울기에 비례하여 parameter를 얼마만큼 업데이트할지 결정하는 수치\n",
        "- 배우는 속도. 데이터와 모델에 따라서 최적값은 모두 다르다.\n",
        "- 보통 실무에서는 초기에 비교적 높은 lr로 시작하여 점차 낮추는 전략을 취함. 단 오히려 batch size를 늘리는 게 더 좋다는 연구도 있음\n",
        "- Learning rate가 너무 크면?? 발산해버리게 된다. Cost가 무진장 늘어난다.\n",
        "- Learning rate가 너무 작으면? cost 값이 변함이 없는 것을 알 수 있다.\n",
        "- lr_scheduler: StepLR, ExponentialLR, MultiStepLR이 있음. 밑에 pytorch method에 설명이 있음\n",
        "\n",
        "> Loss function\n",
        "- data point에서의 오차. single data set에서의 오차를 구하는 것\n",
        "- 순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n",
        "- A loss function **is a part of** a cost function **which is a type of** an objective function!!!\n",
        "\n",
        "> Neural Network\n",
        "- 뇌에 있는 neuron의 모양을 본 따서 만든 신경망\n",
        "- 여러 자극 혹은 입력이 들어오면 각각 가중치를 곱해 더해주고 추가적으로 편차를 더한 값을 activation function을 통해 변형하여 전달하는 네트워크!\n",
        "- f는 activation function  \n",
        "$y = w_4(f(w_3(f(w_2(f(w_1*x+b_1))+b_2))+b_3))+b_4$\n",
        "\n",
        "> Normalization (정규화)\n",
        "- 데이터를 분포를 맞춰주는 역할을 한다.\n",
        "- 정규화 방법: Standardization(정규화), minmax(최소극대화)\n",
        "  - Standarization: 주어진 평균과 std로 데이터의 분포를 바꿈\n",
        "  - minmax: 모든 데이터를 0~1로 바꿔줌. 평균적 범위를 넘어서는 너무 작거나 큰 이상치가 있는 경우에는 오히려 학습에 방해가 되기도 함.\n",
        "- 정규화를 하면 일반적으로 학습이 더 잘된다 -> 원형에 가까운 형태로 손실 그래프가 바뀌어서 불필요한 업데이트가 줄어들고더 큰 lr을 적용할 수 있기 때문\n",
        "\n",
        "> Optimization\n",
        "- Regular equation을 쓰지 않고 optimization을 진행하는 이유: regular equation을 사용하면 데이터 크기가 커질수록 복잡도가 증가하여 계산이 비효율적이고 확정상이 좋지 않기 때문.\n",
        "[Optimization Algorithms](https://medium.com/analytics-vidhya/optimization-algorithms-for-deep-learning-1f1a2bd4c46b)\n",
        "- Given an algorithm f(x), an optimization algorithm help in either minimizing or maximizing the value of f(x). In the context of deep learning, we use optimization algorithms to train the neural network by optimizing the cost function J[J(W,b)]\n",
        "![optimization 변천사](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)\n",
        "- 스탭방향: Gradient, 스탭사이즈: Learning rate\n",
        "- GD: 모든 데이터를 검토한 뒤 방향을 찾자\n",
        "- SGD: 조금씩 데이터를 검토한 뒤 자주 방향을 찾자\n",
        "- Momentum: 관성 개념을 도입해서 덜 비틀거리면서 가보자\n",
        "- Adagrad: 처음엔 빠르게 학습하고 나중에 세밀하게 학습하자\n",
        "- NAG: 관성방향으로 먼저 움직인 뒤 계산한 방향으로 가보자\n",
        "- RMSProp: 세밀하게 학습하되 상황을 보며 정도를 정하자\n",
        "- AdaDelta: 세밀한 정도가 너무 작아져서 학습이 안되는 것을 막자\n",
        "- Adam: 둘 다 고려해서 방향을 찾자\n",
        "\n",
        "> Overshooting\n",
        "- Leaerning rate이 너무 크면 diverge 하면서 cost가 점점 늘어난다.\n",
        "\n",
        "> Padding Method\n",
        "- 여러 개의 sequence data를 하나의 batch로 묶는 방법 중 하나\n",
        "- 가장 긴 sequence의 길이에 맞춰 나머지 data의 뒷부분을 pad로 치환하는 방법\n",
        "- 단점: 계산하지 않아도 될 부분이 늘어난다.\n",
        "\n",
        "> Packing Method\n",
        "- 여러 개의 sequence data를 하나의 batch로 묶는 방법 중 하나\n",
        "- sequence 길이를 기억하는 방법으로 저장.\n",
        "- 단점: 길이 내림차순으로 정렬이 되어야 pytorch에서 동작할 수 있다. padding보다는 구현이 좀 더 복잡하다.\n",
        "\n",
        "> Perceptron\n",
        "- Neural Network의 한 종류\n",
        "- x가 입력되었을 때 x에 w(가중치)를 곱하고 bias를 더하고서, activation function(e.g. sigmoid function)을 거쳐서 최종적인 output을 만든다. \n",
        "- 초창기는 `linear classifier`을 위해서 만들어진 모델\n",
        "- AND gate & OR gate & XOR gate\n",
        "\n",
        "|A|B|AND|OR|XOR|\n",
        "|--|--|--|--|--|\n",
        "|0|0|0|0|0|\n",
        "|0|1|0|1|1|\n",
        "|1|0|0|1|1|\n",
        "|1|1|1|1|0|\n",
        "\n",
        "> Pooling\n",
        "- 상황에 따라 초고화질이 필요하지 않을 수도 있고, 좀 넓게 봐야 파악할 수 있는 특성이 있는데 그때 pooling을 사용!\n",
        "- 이미지 사이즈를 줄이기 위해서 사용되기도 하고, fully connected 연산을 대체하기 위해서 average pooling을 사용하기도 한다.\n",
        "- Max Pooling: n*n에서 가장 큰 값이 반환\n",
        "- Average Pooling: n*n안에서 평균이 반환\n",
        "\n",
        "> Regularization\n",
        "- 제약 조건을 추가로 걸어줌으로써 overfitting을 해결하는 기법\n",
        "- MSE 말고도 람다와 변수의 합을 곱한 항(정형화 식)이 추가되어있음.\n",
        "- 전체 식을 최소화하려면 정형화 식 부분도 작아져야 함. Penalty를 주는 것 이를 통해 함수의 형태가 단순해지고 overfitting도 줄어들게 되는 것.\n",
        "- 한편 여기서 람다가 너무 커지면 w의 값이 매우 작아져 함수가 너무 단순해지는 underfitting 발생 -> 람다의 적절한 값을 찾아야 함.\n",
        "- L1 Regularization  \n",
        "$w^*=\\underset{w}{\\operatorname{argmin}}\\sum_{j}(t(x_j)-\\sum_{i}w_ih_i(x_j))^2 + \\lambda\\sum_{i=1}^k\\lvert{w_i}\\lvert$\n",
        "  - w 값들이 0이 되는 경우가 많다.\n",
        "  - 0이 나왔다는 것은 특정 feature가 오차를 줄이는 데 영향이 없다는 것을 의미\n",
        "  - 이를 활용해서 feature selection을 하기도 함.\n",
        "- L2 Regularization  \n",
        "$w^*=\\underset{w}{\\operatorname{argmin}}\\sum_{j}(t(x_j)-\\sum_{i}w_ih_i(x_j))^2 + \\lambda\\sum_{i=1}^k{w_i^2}$\n",
        "  - 수식의 해를 구할 수 있다. w값이 0이 되기 어렵다.\n",
        "\n",
        "> ReLU  \n",
        "- Vanishing Gradient 문제를 해결하는 activation function\n",
        "- 단점: 어느 순간 큰 손실이 발생해 w와 bias가 마이너스로 떨어지는 경우, 어떠한 입력값에도 활성화 값이 0이 되는 dying neuron 현상이 발생\n",
        "- $f(x) = max(0,x)$\n",
        "\n",
        "> Sequence Data\n",
        "- 순서가 존재하는 데이터\n",
        "\n",
        "> Sigmoid function\n",
        "- $S(x) = \\frac{1}{1 + e^{-x}}$\n",
        "- 실함수로써 유계이며 미분가능, 모든 점에서의 미분값은 양수이기에 역전파에 유용\n",
        "- 한계: Gradient를 계산하면서 문제가 발생. 양 끝 부분에서 gradient를 구하면 값이 매우 작아진다. backpropagation을 통해서 gradient를 전파시킬 때, activation function에서 gradient를 곱하게 된다. 작은 값이 곱해지면서 loss로부터 전파되는 gradient가 소멸되는 문제가 발생 (Vanishing Gradient)\n",
        "\n",
        "> Skip-gram\n",
        "- embedding의 대표적인 방식\n",
        "- CBOW와는 반대로 중심 단어로부터 주변 단어들이 나오도록 모델을 학습하여 embedding vector를 얻는 방식.\n",
        "\n",
        "> Softmax  \n",
        "- $softmax(y_i) = \\frac{exp(y_i)}{{\\sum_{j}exp(y_j)}}$\n",
        "- 입력받은 값을 출력으로 0~1 사이의 값으로 모두 정규화하여 출력값들의 총합은 항상 1이 되는 함수\n",
        "- Logistic regression이 binary class 예측이면 softmax는 더 다양한 class 예측가능\n",
        "- CNN 신경망의 결괏값을 확률로 바꿔줘야 할 때 사용하는 함수\n",
        "\n",
        "> Super resolution\n",
        "- 저화질의 이미지를 입력으로 받아서 고화질로 변환하는 작업\n",
        "\n",
        "> Tensor\n",
        "- n차원의 배열을 전부 포함하는 개념\n",
        "\n",
        "> Torchvision\n",
        "- 다양한 dataset과 model을 제공해주는 패키지다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uMGUztimedg",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch 메소드 모음\n",
        "```메소드참조```부분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTXVtk-mmk7f",
        "colab_type": "text"
      },
      "source": [
        "> 기초 모델 관련\n",
        "- **.ToTensor ( )** : 이미지 순서와 값들을 pytorch에 맞게 바꿔줌\n",
        "- **torch .tensor ( data = , requires_grad = True)** : requires_grad는 tensor에 대한 기울기를 저장할지 여부\n",
        "- **.item()** : Tensor 형태에서 앞에 item만을 반환\n",
        "- **.zero_grad ( )** : gradienet를 초기화\n",
        "- **.backward ( )** : Backpropagation 진행\n",
        "- **.step ( )** : parameter를 업데이트!\n",
        "- **torch .no_grad ( )** : 테스트를 진행하는데 기울기를 계산하지 않겠다는 것\n",
        "- **torch .optim .lr_scheduler .StepLR** : 정해진 step_size(epoch 수)마다 lr에 gamma를 곱해 lr을 감소시킴\n",
        "- **torch .optim .lr_scheduler .ExponentialLR** : 매 epoch 마다 lr에 gamma를 곱해 감소\n",
        "- **torch .optim .lr_scheduler .MultiStepLR** : step_size에 milestones 인수에 list로 받아서 원하는 지점마다 lr을 감소\n",
        "- **torch .nn .BatchNorm1d ( num_features , eps = 1e-05 , momentum = 0.1, affine = True, track_running_stats = True)**: batch normalization하는 code\n",
        "\n",
        "> CNN 관련\n",
        "- **torch .nn .Conv2d (in_channels, out_channels, kernel_size, stride = 1, padding = 0, bias  = True)** : 이런식으로 입력 채널, 출력채널, 커널 크기 등에 따라서 convolution을 진행할 수 있음\n",
        "- **torch .nn .MaxPool2d ( kernel_size, stride = None, padding = 0, dilation =1, return_indices = False, ceil_mode = False)** : MaxPool 설정하는 법\n",
        "- input = (Batch_size, in_channel, height, width)\n",
        "- output = (Batch_size, out_channel, height, width)\n",
        "\n",
        "> RNN 관련\n",
        "- **rnn .init_hidden( )** : 학습을 시작하려면 순환 신경망 은닉층의 초깃값을 지정하기 위함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ6HbDW9C905",
        "colab_type": "text"
      },
      "source": [
        "# 0. DNN (Deep Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZSf-RrICfkv",
        "colab_type": "text"
      },
      "source": [
        "## DNN이란?\n",
        "![](https://static.wixstatic.com/media/a27d24_c1620508d5064be294c79bdee5310f22~mv2.png/v1/fit/w_1608,h_798,al_c,q_80/file.webp)\n",
        "- 인공 신경망에서 hidden layer가 2개 이상인 네트워크\n",
        "- 지도학습, 비지도학습, 강화학습 모두에 사용됨\n",
        "> DNN을 훈련하는 과정\n",
        " 1. Nerural Network 구조를 만든다.\n",
        " 2. 구조를 train한 다음에 성능을 평가한다.\n",
        " 3. Overfitting이 되었다면 regularization(e.g. drop-out, batch-normalization) 등을 사용한다.\n",
        " 4. 최고의 성능을 내기 위해 위 과정을 반복한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc2C22R1nDvn",
        "colab_type": "text"
      },
      "source": [
        "## DNN 과정 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xddvenpDHok",
        "colab_type": "text"
      },
      "source": [
        "> Step.1\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "\n",
        "> Step.2\n",
        "- Data 불러오기\n",
        "\n",
        "> Step.3  \n",
        "- 1) Input data를 Linear layer을 통과시킨 뒤 non-linear activation function을 통과시켜서 output을 구한다.  \n",
        "- 2) 위에서 나온 비선형 output을 다시 다음 Linear layer를 통과시킨뒤 non-linear activation function을 통과시켜 output을 구한다.  \n",
        "- 3) 위의 과정을 반복해서 최종 output을 뽑아준다.\n",
        "- 모델의 효율을 높이기 위해서 weight initilization, batch normalization, dropout 등을 사용\n",
        "\n",
        "> Step.4  \n",
        "- 1) 최종적으로 나온 output과 target 값의 차이(오차)를 구해준다.  \n",
        "- 2) 학습하면서 생긴 수많은 parameter(weight 등)가 오차에서 차지하는 비중을 계산한다.  \n",
        "- 3) 그 비중만큼 다시 parameter들을 업데이트해준다.\n",
        "\n",
        "> Step.5  \n",
        "- Step.3에서 Step.4를 반복하면서 parameter를 업데이트 해나간다.  \n",
        "\n",
        "> Step.6\n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfPNPNXFrE-",
        "colab_type": "text"
      },
      "source": [
        "## 하나하나 뜯어봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOHgaMFVFspK",
        "colab_type": "text"
      },
      "source": [
        "### Step.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUMkA3SFwNu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLktu7X3Fzkk",
        "colab_type": "text"
      },
      "source": [
        "GPU는 써야 성능이 오지게 빨라지겠쥬?\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "요로로콤 설정하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irdcEEPdGjHH",
        "colab_type": "text"
      },
      "source": [
        "### Step.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_NX8ZIVGjeB",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "1) Data 불러오기  \n",
        "2) Hyperparameter 설정해주기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsx-dywXG_WQ",
        "colab_type": "text"
      },
      "source": [
        "1) DataLoader를 사용해서 data를 불러주면 된다~  \n",
        "2) learning_rate, training_epochs, batch_size, drop_prob 등을 설정해주기!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jqmrW03GvSk",
        "colab_type": "text"
      },
      "source": [
        "### Step.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYmWIZt0Gjwz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "1) Input data를 Linear layer을 통과시킨 뒤 non-linear activation function을 통과시켜서 output을 구한다.  \n",
        "2) 위에서 나온 비선형 output을 다시 다음 Linear layer를 통과시킨뒤 non-linear activation function을 통과시켜 output을 구한다.  \n",
        "3) 위의 과정을 반복해서 최종 output을 뽑아준다.  \n",
        "- 모델의 효율을 높이기 위해서 weight initialization, batch normalization, dropout 등을 사용\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pudTVE_aUCMJ",
        "colab_type": "text"
      },
      "source": [
        "1) Activation function  \n",
        "- 신경망의 출력을 결정하는 식!  \n",
        "- Sigmoid, ReLU, Leaky ReLU, Hyperbolic Tangent(tanh)  ```딥러닝 관련 용어 참조```  \n",
        "\n",
        "2) 비선형 Activation function을 사용하는 이유\n",
        "- Activation function이 선형이면 layer를 깊게 쌓더라도 결국 선형 변환이기 때문.  \n",
        "  비선형 변환을 해줘야 비선형 관계를 학습할 수 있음!\n",
        "\n",
        "3) 수식으로 보면 요로로콤 \n",
        "$$y = w_4(f(w_3(f(w_2(f(w_1*x+b_1))+b_2))+b_3))+b_4$$  \n",
        "- $w_1$: 첫 번째 input  \n",
        "- f: Activation Function\n",
        "- y: 최종 output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZF7qHzDGuTd",
        "colab_type": "text"
      },
      "source": [
        "### Step.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ING1YlXsGjtW",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "1) 최종적으로 나온 output과 target 값의 차이(오차)를 구해준다.  \n",
        "2) 학습하면서 생긴 수많은 parameter(weight 등)가 오차에서 차지하는 비중을 계산한다.  \n",
        "3) 그 비중만큼 다시 parameter들을 업데이트해준다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG4MYurTYjqj",
        "colab_type": "text"
      },
      "source": [
        "1) Loss function  \n",
        "- Output과 target의 오차 구하기!!  \n",
        "- Loss function는 방법은 CrossEntropy, BinaryCrossEntropy```용어참조```   등을 주로 사용! \n",
        "\n",
        "2) Backpropagation```용어참조```\n",
        "- 각 parameter가 loss에서 차지하는 비중을 chain rule을 이용해서 거꾸로 gradient를 구해준다!\n",
        "\n",
        "3) Optimizer  \n",
        "- Optimizer를 활용하여 2)에서 구한 gradient들을 바탕으로 기존 parameter에 업데이트를 해준다.  \n",
        "- Optimizer로는 Gradient Descent, Stochastic Gradient Descent, Adam```용어참조```   등이 있다. \n",
        "- Adam을 가장 많이 사용한다!\n",
        "- parameter를 업데이트를 할 때는 learning_rate```용어참조```  을 고려해서 업데이트를 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOdk6QUccAwv",
        "colab_type": "text"
      },
      "source": [
        "#### Backpropgation을 풀어쓴 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMb0AXjVYjjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backpropagation Code\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# weight, bias를 직접 선언\n",
        "w1 = torch.Tensor(2, 2).to(devce)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2, 1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)\n",
        "\n",
        "def sigmoid(x):\n",
        "    #  sigmoid function\n",
        "    return 1.0 / (1.0 + torch.exp(-x))\n",
        "    # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\n",
        "\n",
        "def sigmoid_prime(x):    # sigmoid를 미분한 것\n",
        "    # derivative of the sigmoid function\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "for step in range(10001):\n",
        "  # forward\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  a1 = sigmoid(l1)\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  # binary cross entropy loss를 사용\n",
        "  cost = -torch.mean(Y*torch.log(Y_pred) + (1-Y)*torch.log(1 - Y_pred))\n",
        "\n",
        "  # Back prop (chain rule)    # pytorch에서 .backward() 에 해당하는 부분\n",
        "  # Loss derivative\n",
        "  d_Y_pred = (Y_pred - Y) / Y_pred * (1.0 - Y_pred) + 1e-7)    # 1e-7은 0으로 나눠주는 것을 방지하기 위함\n",
        "\n",
        "  # Layer 2\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(12)\n",
        "  d_b2 = d_12\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
        "\n",
        "  # Layer 1\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "\n",
        "  # Weight Update    # pytorch에서 .step() 에 해당하는 부분\n",
        "  w1 = w1 - learning_rate * d_w1\n",
        "  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate * d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-DBIey1G0ha",
        "colab_type": "text"
      },
      "source": [
        "### Step.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdsO22YKGjo7",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- Step.3에서 Step.4를 반복하면서 parameter를 업데이트 해나간다.  \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fM7OXScFTo",
        "colab_type": "text"
      },
      "source": [
        "-.step(): backpropagation을 바탕으로 계산된 gradient를 기존 parameter에 업데이트하여 새로운 parameter를 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEonMRj9i9zQ",
        "colab_type": "text"
      },
      "source": [
        "### Step.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_bYdsfTi_bo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5yboUeBjDx3",
        "colab_type": "text"
      },
      "source": [
        "test는 test지 무슨 설명이 더 필요항가~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNvpr4FadVQL",
        "colab_type": "text"
      },
      "source": [
        "## 간단하게 코드를 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OSPeAmOhiOi",
        "colab_type": "text"
      },
      "source": [
        "### 심플 version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd-TIHxIe2ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "'''\n",
        "torch.nn.Linear(in_features, out_features, bias = True)\n",
        "- in_features: input sample의 사이즈!\n",
        "- out_features: output sample의 사이즈!\n",
        "- bias: 편차\n",
        "'''\n",
        "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
        "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()    # 얘가 Activation function!!\n",
        "\n",
        "# torch.nn.Sequential을 통해서 model을 만들어주는 것!!!\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "criterion = torch.nn.BCELoss().to(device)   # 여기서는 binary cross entropy를 loss function으로 썼음!!\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # Optimizer를 Stochastic Gradient Descent를 씀!!\n",
        "\n",
        "for step in range(10001):\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "    \n",
        "    cost.backward()   # 이 부분이 Backpropagation! .backward()가 backpropagation해주는 메소드!!!\n",
        "    optimizer.step()    # 이 부분이 parameter를 업데이트하는 메소드!!!\n",
        "\n",
        "    if step % 400 == 0:\n",
        "        print(step, cost.item())\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "with torch.no_grad():\n",
        "    hypothesis = model(X)\n",
        "    predicted = (hypothesis > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtUYXOThd3v",
        "colab_type": "text"
      },
      "source": [
        "### Weight를 initialization 해서 학습하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni5WpOtBg7jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "'''\n",
        "Hyperparameter 설정해주기!!\n",
        "'''\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 256, bias=True)    # input이 784개였던 input의 크기를\n",
        "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
        "linear3 = torch.nn.Linear(256, 10, bias=True)   # 마지막에 10개로 줄여준다는 의미!!! (input 데이터의 class가 10개임!)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "# weight initialization (xavier)\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Test the model using test sets\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bCY9Vv-jPWc",
        "colab_type": "text"
      },
      "source": [
        "### Dropout과 initialization을 사용하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwEpIQqqjPCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "drop_prob = 0.3\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
        "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=drop_prob)\n",
        "\n",
        "# xavier initialization\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "torch.nn.init.xavier_uniform_(linear4.weight)\n",
        "torch.nn.init.xavier_uniform_(linear5.weight)\n",
        "\n",
        "# model\n",
        "'''\n",
        "여기서 model의 layer를 어떻게 쌓아나가는지 주모오오오오옥~~~!!!\n",
        "linear -> relu -> dropout\n",
        "'''\n",
        "model = torch.nn.Sequential(linear1, relu, dropout,\n",
        "                            linear2, relu, dropout,\n",
        "                            linear3, relu, dropout,\n",
        "                            linear4, relu, dropout,\n",
        "                            linear5).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "''' 여기서는 Adam optimizer를 사용했음!!!'''\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "model.train()    # set the model to train mode (dropout=True)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Test model and check accuracy\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "with torch.no_grad():\n",
        "    model.eval()    # set the model to evaluation mode (dropout=False)\n",
        "\n",
        "    # Test the model using test sets\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPrTfiO3j0gz",
        "colab_type": "text"
      },
      "source": [
        "### Batch normalization을 사용하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwy0AJdrj0a-",
        "colab_type": "code",
        "outputId": "216a3fd7-17c6-4f77-ab97-d06787cd9b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "drop_prob = 0.3\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "'''\n",
        "torch.nn.BatchNorm1d(num_features, eps = 1e-05, momentum = 0.1)\n",
        ":batch normalization을 하는 feature의 개수를 정해주면 간편간편쓰~!\n",
        "'''\n",
        "bn1 = torch.nn.BatchNorm1d(32)\n",
        "bn2 = torch.nn.BatchNorm1d(32)\n",
        "\n",
        "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "nn_linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "\n",
        "# model\n",
        "bn_model = torch.nn.Sequential(linear1, bn1, relu,\n",
        "                            linear2, bn2, relu,\n",
        "                            linear3).to(device)\n",
        "nn_model = torch.nn.Sequential(nn_linear1, relu,\n",
        "                               nn_linear2, relu,\n",
        "                               nn_linear3).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
        "'''여기서는 Adam optimizer를 사용했음!!!'''\n",
        "\n",
        "# Save Losses and Accuracies every epoch\n",
        "# We are going to plot them later\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "test_total_batch = len(test_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    bn_model.train()  # set the model to train mode\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "        bn_optimizer.zero_grad()\n",
        "        bn_prediction = bn_model(X)\n",
        "        bn_loss = criterion(bn_prediction, Y)\n",
        "        bn_loss.backward()\n",
        "        bn_optimizer.step()\n",
        "\n",
        "        nn_optimizer.zero_grad()\n",
        "        nn_prediction = nn_model(X)\n",
        "        nn_loss = criterion(nn_prediction, Y)\n",
        "        nn_loss.backward()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "        nn_optimizer.step()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "    with torch.no_grad():\n",
        "        bn_model.eval()     # set the model to evaluation mode\n",
        "\n",
        "        # Test the model using train sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(train_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\n",
        "\n",
        "        # Save train losses/acc\n",
        "        train_losses.append([bn_loss, nn_loss])\n",
        "        train_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        # Test the model using test sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(test_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n",
        "\n",
        "        # Save valid losses/acc\n",
        "        valid_losses.append([bn_loss, nn_loss])\n",
        "        valid_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        print()\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfvhxoLWmVPV",
        "colab_type": "text"
      },
      "source": [
        "# 0. CNN (Convolutional Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVYbnpE1nV2L",
        "colab_type": "text"
      },
      "source": [
        "## CNN이란?\n",
        "![](https://taewanmerepo.github.io/2018/01/cnn/head.png)\n",
        "> 중요 특징\n",
        "- CNN은 이미지의 feature를 추출하는 모델이다!\n",
        "- input에 대해 몇 번의 convolution 연산을 activation function과 함께 적용한 이후 pooling으로 전체 크기를 줄여주는 과정을 반복 -> 필터가 이미지를 지나가면서 이미지의 부분 부분이 필터와 얼마나 일치하는지 계산 -> 이를 통해서 이미지의 feature를 추출함\n",
        "\n",
        "> 기타 특징\n",
        "- Feature extraction에서 channel을 늘려서 학습한 뒤 classification에서 channel을 줄여서 output으로 바꿔줌!\n",
        "- CNN은 parameter를 공유하여 전체 parameter 수를 줄여 overfitting을 줄여줌\n",
        "- 하나의 이미지에 같은 필터를 연달아 적용하여 가중치가 공유. 그래서 기존 neural network보다 학습의 대상이 되는 변수가 적다!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9HBT10VDLsV",
        "colab_type": "text"
      },
      "source": [
        "## CNN 과정 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwPuIaPzDMQn",
        "colab_type": "text"
      },
      "source": [
        "> Step.1\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "\n",
        "> Step.2\n",
        "- 1) Data 불러오기\n",
        "- 2) Hyperparameter 설정해주기\n",
        "\n",
        "> Step.3  \n",
        "- 1) Layer 만들어주기: Convolution 연산, Activation Function, Pooling  \n",
        "- 2) Funnly connected layer로 모아주기  \n",
        "- 3) 모델 설정해주고 Loss function, Optimizer 설정해주기\n",
        "\n",
        "> Step.4  \n",
        "- 모델 Train하기  \n",
        "\n",
        "> Step.5  \n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mN_CYG-3HTOD"
      },
      "source": [
        "## 하나하나 뜯어봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gm6CisLiHTOF"
      },
      "source": [
        "### Step.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4azL2mQSHTOG"
      },
      "source": [
        "---\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BcYAGOVMHTOH"
      },
      "source": [
        "GPU는 써야 성능이 오지게 빨라지겠쥬?\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "요로로콤 설정하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PA2pDhTBHTOH"
      },
      "source": [
        "### Step.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Hz1Wvw2HTOI"
      },
      "source": [
        "---\n",
        "1) Data 불러오기  \n",
        "2) Hyperparameter 설정해주기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YX-bQFaWHTOJ"
      },
      "source": [
        "1) DataLoader를 사용해서 data를 불러주면 된다~  \n",
        "2) Learning rate, Training_epochs, batch_size 등을 설정해주면 됨!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BKVSlh3lHTOK"
      },
      "source": [
        "### Step.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W4qV9dA_HTOL"
      },
      "source": [
        "---\n",
        "1) Layer 만들어주기: Convolution 연산, Activation Function, Pooling  \n",
        "2) Fully connected layer로 모아주기  \n",
        "3) 모델 설정해주고 Loss function, Optimizer 설정해주기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X5jhCX5OHTOM"
      },
      "source": [
        "1) Layer 만들기\n",
        "- Layer는 크게 Conv2d(Convolution 연산), Activation Function, Pooling을 사용한다.\n",
        "- Conv2d 메소드에 대한 설명은 위에 ```메소즈참조```를 참조\n",
        "- Convolution 연산을 통해서 filter와 이미지가 얼마나 일치하는지 보고 -> 비선형 변환 -> Pooling으로 그림의 크기를 줄여주는 과정의 반복!\n",
        "- 즉! 크기는 줄이고! Channel은 늘리고!!\n",
        "  - Channel을 늘려주는 이유: 같은 채널을 여러 겹으로 쌓는 방식! 이를 통해서 채널 수를 늘려서 여러 개로 만든 다음에 이것을 다시 수합해서 얻은 결과로 학습하기 위함\n",
        "- input과 1st hidden layer에 필터가 지정한 개수만큼 학습되고, 이를 통해 다양한 특징들을 뽑아내 다음 층으로 전달!!! 필터를 통해 뽑은 이런 특성들이 중첩됨에 따라 모델은 더 복잡하고 다양한 형태를 구분하는 거임!!!\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "2) Fully connected layer\n",
        "- Fully connected layer(FC)는 ```용어참조``` 확인\n",
        "\n",
        "![](https://taewanmerepo.github.io/2018/01/cnn/head.png)\n",
        "- Layer를 만들어 통과시키면서 channel을 늘리는 건 위 그림에서 Feature extraction 부분에 해당!\n",
        "- Feature extraction에서 channel을 늘리고 classification에서 늘린 channel을 줄여서 output으로 바꿔주는 것!\n",
        "- Channel을 줄일 때 fully connected layer를 사용\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "3) Loss function, Optimizer\n",
        "- Loss function은 보통 BCE, CE를 사용하고 Optimizer는 Adam을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9MlBrxX-HTON"
      },
      "source": [
        "### Step.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lYMhJj0MHTOO"
      },
      "source": [
        "---\n",
        "- 모델 Train하기 \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yzJIFuO8HTOP"
      },
      "source": [
        "- 모델을 train하는 과정은 DNN과 유사\n",
        "- Loss function을 구하고, Backpropgation을 진행한 뒤에, Optimizer를 통해서 parameter를 업데이트해주기!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IJOYGnWxHTOT"
      },
      "source": [
        "### Step.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDT-6wqmHTOU"
      },
      "source": [
        "---\n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!! \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvqY3r_NHTOV"
      },
      "source": [
        "test는 test지 무슨 설명이 더 필요항가~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUHDAlyP9vN9",
        "colab_type": "text"
      },
      "source": [
        "## 간단하게 코드를 살펴봅시다~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7_UjkYL9y3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777) # random value를 고정해주는 코드\n",
        "if device =='cuda':\n",
        "  torch.cuda.manual_seed_all(777)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train = True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root = 'MNIST_data/',\n",
        "                        train = False,\n",
        "                        transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "\n",
        "# data_loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
        "batch_size = batch_size,\n",
        "shuffle = True,\n",
        "drop_last = True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1,32,kernel_size=3, stride=1, padding=1),   # 합성곱 연산하고\n",
        "        nn.ReLU(),    # Activation function을 통과하고\n",
        "        nn.MaxPool2d(2)   # Pooling 중에서 Maxpooling을 진행!\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(32,64, kernel_size=3, stride = 1, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.fc = nn.Linear(7*7*64, 10, bias=True)\n",
        "    torch.nn.init.xavier_uniform(self.fc.weight)    # Xavier로 weight initialization을 진행\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "\n",
        "    out = out.view(out.size(0),-1)  # 다음 fully connected layer에 넣어기 위해 1줄로 flatten해서 shape을 맞춰주는 것이다.\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)    # Cross entropy로 loss function을 계산\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)    # Adam Optimizer 사용!\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# training\n",
        "total_batch = len(data_loader)\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "\n",
        "  for X, Y in data_loader:\n",
        "    X = X.to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    \n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += cost / total_batch\n",
        "\n",
        "  print('[Epoch:{}] cost = {}'.format(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "''' Test할 때는 no_grad() 하는 거 잊지 말기!!!'''\n",
        "\n",
        "with torch.no_grad():\n",
        "  X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "  Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "  prediction = model(X_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy: ', accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A8pURKEJPNKh"
      },
      "source": [
        "# 0. RNN (Recurrent Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bFmKaMvTPNKk"
      },
      "source": [
        "### RNN이란?\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "> 중요특징\n",
        "- 순서가 있는 데이터에서 의미를 찾아내기 위해 고안된 모델\n",
        "- RNN은 음성인식, 기계번역, 목소리로 성별 분류 등에 사용함\n",
        "\n",
        "> 기타특징\n",
        "- RNN은 모든 cell이 parameter를 공유한다.\n",
        "- 같은 순환 내에서 각 Hidden Layer의 함수 및 parameter들은 같은 값을 공유하며, 이들과의 연산으로 나온 각 step 별 Hidden state를 계산해줌\n",
        "- RNN은 Sequence가 길어질수록 parameter들이 업데이트 되며 Vanishing Gradient / Exploding Gradient 문제 발생 \n",
        "\n",
        "> Input data의 형태\n",
        "  - batch_first=True: (Batch Size, Sequence Length, Input Dimension)\n",
        "  - batch_first=False: (Sequence Length, Batch Size, Input Dimension)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L_TaEX40PNKm"
      },
      "source": [
        "## RNN 과정 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1_kqxfUPNKo"
      },
      "source": [
        "> Step.1\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "\n",
        "> Step.2\n",
        "- Data를 setting 해준다.\n",
        "\n",
        "> Step.3  \n",
        "- 1) 모델을 설정해준다.\n",
        "- 2) Loss function, Optimizer를 설정해준다.\n",
        "\n",
        "> Step.4  \n",
        "- Train 해주자아아아ㅏㅏㅏ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Owxe5wyaPNKq"
      },
      "source": [
        "## 하나하나 뜯어봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eRIlvUjePNKs"
      },
      "source": [
        "### Step.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txY-6mAgPNKv"
      },
      "source": [
        "---\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sm_Rz9i5PNKy"
      },
      "source": [
        "GPU는 써야 성능이 오지게 빨라지겠쥬?\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "요로로콤 설정하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKgukolaPNK0"
      },
      "source": [
        "### Step.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSkRbp7GPNK2"
      },
      "source": [
        "---\n",
        "- Data를 setting 해준다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yFtqP7lXPNK7"
      },
      "source": [
        "- 데이터를 dictionary 형태로 바꿔준다!!\n",
        "- input data와 hidden data를 설정해준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-9NMU3GOPNK-"
      },
      "source": [
        "### Step.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ohOA9AB0PNLB"
      },
      "source": [
        "---\n",
        "- 1) 모델을 설정해준다.\n",
        "- 2) Loss function, Optimizer를 설정해준다.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nX9mZ0p_PNLD"
      },
      "source": [
        "1) RNN model  \n",
        " - input dimension과 hidden dimension 그리고 layer를 설정해준다.\n",
        " - fully connected layer도 설정해준다.\n",
        "\n",
        "2) Loss function, Optimizer\n",
        " - Loss function으로는 보통 Cross entropy, Optimizer로는 Adam을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MIGdnIpnPNLF"
      },
      "source": [
        "### Step.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QKVHEY68PNLH"
      },
      "source": [
        "---\n",
        "- Train 해주자아아아ㅏㅏㅏ.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v5qTO-irPNLJ"
      },
      "source": [
        "- input data를 넣어서 RNN network를 통과시킨다.\n",
        "- loss를 구하고 backpropagation으로 gradient를 구한 뒤 optimizer로 parameter를 업데이트해준다!\n",
        "- 바로바로 예측 결과를 출력해 얼마나 모델이 나아지는지 살펴본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Md_SPwBgPNLq"
      },
      "source": [
        "## 간단하게 코드를 살펴봅시다~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pA6Y9Z9FPNLx",
        "outputId": "316b8d24-5385-464e-8f2d-caf90e12603a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "# make dictionary\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "char_dic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 5,\n",
              " \"'\": 2,\n",
              " ',': 10,\n",
              " '.': 24,\n",
              " 'a': 1,\n",
              " 'b': 23,\n",
              " 'c': 18,\n",
              " 'd': 16,\n",
              " 'e': 17,\n",
              " 'f': 3,\n",
              " 'g': 6,\n",
              " 'h': 14,\n",
              " 'i': 0,\n",
              " 'k': 9,\n",
              " 'l': 12,\n",
              " 'm': 4,\n",
              " 'n': 21,\n",
              " 'o': 13,\n",
              " 'p': 15,\n",
              " 'r': 11,\n",
              " 's': 7,\n",
              " 't': 20,\n",
              " 'u': 19,\n",
              " 'w': 22,\n",
              " 'y': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDTBlhamUaKY",
        "colab_type": "code",
        "outputId": "b86c5731-ce96-471e-e2f4-83b9cab63eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# hyper parameters\n",
        "dic_size = len(char_dic)\n",
        "hidden_size = len(char_dic)\n",
        "sequence_length = 10  # Any arbitrary number\n",
        "learning_rate = 0.1\n",
        "\n",
        "# data setting\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
        "\n",
        "# one_hot_vector로 만들어주기\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRPmcAarUaWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform as torch tensor variable\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUPsFqsuUabm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# declare RNN + FC\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2)\n",
        "\n",
        "# loss & optimizer setting\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5dzRTCeUpf7",
        "colab_type": "code",
        "outputId": "01a73452-f734-4219-e181-0ae237ec6816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# start training\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        # print(i, j, ''.join([char_set[t] for t in result]), loss.item())\n",
        "        if j == 0:\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else:\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lsfoocslllsffolofossfffslslolfflssflolcocslllolsssfollsololfolososssslooolffflffclssfoollsllsololslllllfllfooolofofslolsololfllollsolslsolooslfcoolfolllllssslfocoolslsoloslsollsll\n",
            " o ooo ok omoomoomok omomo omoo oooo ooookooo ooo ooko oo ooo omoo  ooo omoo  moo oooo  oo oomo oom  omo omoo ooooo oom mokoookomoomo ooo oo o oo oomouokomo ooo  o omo oom omo om \n",
            "   o o o d i  oio   i   ooooi   ioo       i   i  d      o i   i o  i i   oo   io  i  o  i       io  i o   io oi  o  oo o  oo oii o    i  o  o i   io  i   i ioio o o i  o  io o    \n",
            "  kl  kl  k   s                 s             t      ks  s    s  k   s       kt   s         s  st  s   s     ss  s   s         t  s          k   kt   s   s   s  s   s      s      \n",
            "t t t t t tt t t tm t  t  t t t ttt t tt   tttt t tt t t tt ptt tttt  t    t  t t t t tmtt    ttt t t  t t  t t t tt  t tt t stt tttt tt tt  t tp t t t t tt  t t t tttt t t tmt t \n",
            "n t n t t tt ttt ttttttt tt t t ttt ttttt ttttttttnt ttt ttnttttttttttt t  t tt ttt t ttttt tttttptntttnnttnt t t tt  t tt tnttttttttttt tttdtetttt t t ttttttt t t tttttt t ttt tt\n",
            "n  on                                                      e       s                                a      e         t                    y    y               o       l           \n",
            "en o t o t to  eo t te opo o o t o o o   t t    n oit   n te  e    te on ttetoeo t o o t   t t      teto o o t o o tto t nt  n      o   n o t  o l o o o t o o o t t  et  t o e oio\n",
            "e  o t tetoelet mopolo o o   oetooeo teooloeo oom oelooom tooloo oomoel o molo oetomom t o tomo oo  roto t oot t tem  otop oom oo oel oomtootopo o mel o te  t o lolooo oototeo o t\n",
            "tstoa  o   t  lt lo     aoa  oa    o  goo      o   m   g   o  oe o  o o l  osoeoa       go  o   ug    to  goa    oa g      to   o ow  oo  oe  g a t g     g   goa    oom    og     \n",
            "trtot ton teot    totod tod ton t   s t o  t   o   a o o  to  o to   to   tos tot t   s tos o  doo  totod dot t tot t o o s o  oodo od o to   to  t t dos o s tot     ot o to  d   \n",
            "t.tot te s tm to he tmd too ton tot    todho   o       o  to to th t eon  t s tms t t s thntoe d o  t dos dhn t tot t   t   o  todh  d o  o   om  t e t   o   trt     mt   to  t  t\n",
            "t.tor ttn  tottoehe t d eot tor t to  eeodte   ent     o  theto t  tttor  tod tor t t d tmn ee d o  t dod don t tor t d t   o  totot d o tt   tor t e t d en  totl   ape    h  d   \n",
            "t.tor t n ttoetoe t d d dod doe t to  ttod e   endoe t e  t ett  t   toe ttod tor t tod tnd ee d e  t dod dor t tor t d tnd o  t dor d e tt  ttoe t e tod and tor    ent e toe doe \n",
            "t.tor ton ttoebt  e d d dod doe t to   ee le   e doe t en toete  t   to  ttnd toe t tod tnd e  d e  t dod tor t toe t d en te 't doe d e tt  taoe toe dod a d torle tentpe toe doe \n",
            "t.tor ton tto bo le d d dod doe't to   to o   te dor t en toebe le l won ttns wor't ao  wndt e  te  d dod wor t toe e  len to 't doe  te te   aoe toe do  e d wodle ttntoe toe doe \n",
            "ontor tonsewonbo ee d d do, don't to l to o   le ton lten toebe le t won ttns wor't e s wndthe  th  d dos won't toe e  len to 'w doe  to be   eon'toend   ens wodon ltntoe toe do  \n",
            "ontor tons wonbo ee t t to, don't ao   toeee  le ton   en toeke le t ton ttns won't t s wnsthe  ths d dns won t the t   en toa t toe  toeke   aon the t s e s wogon' tyeoe the d   \n",
            "ontor tonshaontusee t t to, don't ao h toeeen le thn   en toeko le t ton ttns tonkt tns ensthe  tos d tns ton t the e she  to  t the  toele   aon the t s e s iodpn' tyehe the t   \n",
            "ontorltonshao tusee t dhtod don't ao e theeen le th l hen toeco lo thtonl tns don't ans instheoetos t dns don t thehenshen to  t them toecorlhton the dh he stioepn' aytoa the d   \n",
            "pytonltonthao bus e t dheoa don't ao h to eenece to l hen tonlonle t tonl tns don't ans ansthe etos t t s don t thd endhen to ct toem torlo lhton the do he s iorpn' aydof toe d   \n",
            "nytonltont donbus e t dhdma don't aoro todeeoele to l her to lonle t donl tnd don't t d fn toe etos t t d don t dhr tnd en to ct toe  toglo l don the dod e s iyron' tydof the d d \n",
            "pytorlte t do buspe tndhrme don't aorp todeeoele to l her toelonle t ton' and don't a dirn toe etos s tnd donkt dhd tndher to lt the  to lo l don the dhd e s iyrpn' iydof themd  c\n",
            "pytonltent wo busee a dhrmw don't aorl to peoele to   her toulonle t ton' ans won't a siwn toe etos s and workt thd enshem to lt ahe  to lo l won the dhd e   imnpn' iy ef themd  c\n",
            "pyton test wo bus e t dhdpw don't aorl to people to   ler to lople t ton' tns won't a siwn tee etos shans workt wus enshem tosct ahem to torl won the dos e s imner' iy of the dhac\n",
            "pyworlton' wo busee a dhdp, don't aorl tr people to lther to conlect ton' ans don't ansitnsther toscs ans dork, wus enshem to ct ahem toalonc wonkthe  os ens imnen' iy of the dsic\n",
            "pyworl,on' wo buiee a dhep, don't aoul toeeeople to lthe  to conlent tond and don't a sirnsther toscs and dork, dud t dher to ct ahe  to lonc won the thd e s immen' ty of the tsic\n",
            "pyworltont wo buiee a dhip, don't aorr tp eeople to ether to lonlent dond and don't a signsther toskd and dork, dus tndher to kt ahe  to long ton the erd e s immen' iy of the dsic\n",
            "pywo ltont fo bui e d dhim, don't aoum to people to lther to tonlect wond and don't assignsther tosksiand dork, wus tndher to kh them to bong for them nd ens immen' iy of toemesis\n",
            "pybooltont do buile a dhip, don't aoum tp peophe to lther to collect aond and dor't a sign them tosks and dork, wui rasher to kh them to cong for the end e siimmpn' ty of the esik\n",
            "pybooltont ao built a dhip, dor't aoum tp people to ether to collect wond and don't a sign the  tosks and dork, dud rasher to kh the  to cong eor the end e s immpn' ty of the ehic\n",
            "pybooltant wo buald a ship, don't aoum tp people to lther to collect wond and won't a sign them toskt and work, wum rasher to ch them to bong tor the  nd e s immpnd ty of the dhic\n",
            "pywooetant to buiie a ship, don't aoum tp people to ether to collect woad and won't a sign them tosks and wors, wud radher to ch them to ceng wor the  nd e s immpns ty of the dhic\n",
            "py,ooetant wo lui d a ship, don't aoum tp people to ether to collect wood and won't ansign them tosks and work, wud radher to ch them to long fon the end e s immens ty of the ehic\n",
            "py,oo tant ao build a dhip, don't aoum tp people to ether to lollect wood and don't assign them tosks and dork, bud radher to ch them to long for the end e s immens ty of the enak\n",
            "tm,oo tant to build a dhip, don't aoum tpepeople to ether te collect woad and don't assign them tosks and dork, bud radher th kh them to long for the dnd ess immens ty of the dnak\n",
            "tm,ooetant to buile a dhip, don't aoum tp people to ether te collect wood and won't assign them tosks and work, but rather te ch them to long for the endless immensity of the enis\n",
            "tm,ooetant to luild a dhip, don't aoup up people to ether te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the ehac\n",
            "tm,oo want to buiid a ship, don't aoum tp people to ether to collect wood and won't assign them tosks and work, but rather toach them to long for the endless immensity of the ehac\n",
            "tn,oo want to build asship, don't aoum up people to ether to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the ehac\n",
            "tn,oo want to build a ship, don't aoum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the dhae\n",
            "tn,oo want to build a ship, don't aoum up people together to collect wood and don't a sign them tosks and dork, but rather toach them to long for the endless immensity of the ehae\n",
            "tn,oo want to build a ship, don't aoum up people together to collect wood and won't a sign them tosks and work, but rather toach them to long for the endless immensity of the dhae\n",
            "tn,oo want to build a ship, don't arum up people together to collect wood and won't a sign them tosks and work, but rather teach them to long for the endless immensity of the drac\n",
            "tn,oo want to build a ship, don't arum up people together te collect wood and won't a sign them tasks and work, but rather teach them ta long for the endless immensity of the dhac\n",
            "tn,oo want to build a ship, don't arum up people together te collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the shac\n",
            "tnyoo want to build a ship, don't arum up people together te collect wood and won't ansign them tasks and work, but rather teach them ta long for the endless immensity of the srac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the dhac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the drac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sra.\n",
            "tiyouewant to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the srac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sra.\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sha.\n",
            "tnyouewant to build a ship, don't arum up people together teacollect wood and won't ansign them tasks and work, but rather teach them to long for themendless immensity of the erac\n",
            "tnyou want to build a dhip, don't aoum up people together to collect wood asd don't assign ther tasks and dork, but rather toach them to long for the sndless immensity ef the srai\n",
            "tiyou want to build a ship, don't arui up people together to collect wood ans son't assign ther tosks ans sork, but rather toach them to long for the snd ess immensity of the shac\n",
            "lnyou want to build asship, don't arum up people together to collect eord and won't assign them tosks and work, but rather toach them to bong for the endless immensity of the erad\n",
            "tryou want to build d ship, don't arum up people together te collect wood and won't ans gn them tosks and work, but aather teach them to cong for the endless immensity of the sra.\n",
            "tyyou want to build a ship, don't arum up people together te collect wood and won't ass gn them tosks and work, but rather teach them to long for the endless immengity of the shad\n",
            "ty,ou want to build asship, don't arum up people together to collect wood and won't assign them tasks and dork, but rather toach them ta long for the endless immensity of the shad\n",
            "ty,ou want to build asship, bon't arum up people together to collect wood and don't assign ther tosks and dork, but rather toach them to long for the endless immensity of the elad\n",
            "ty,ou tant to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the elad\n",
            "tysou tant to build a ship, don't arum up people together te collect wood and don't dssign them tosks ind dork, but rather teach them to long for the sndless immensity of the sla.\n",
            "tysou want to build a ship, don't arum up people together te bollect wood and won't assign them tosks ind work, but rather teach them to long for the endless immensity of the sla.\n",
            "pnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sla.\n",
            "pnyou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "lnyou want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the slan\n",
            "lnyou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the slan\n",
            "lnyou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and dork, but rather teach them to long for the endless immensity of the sla.\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the slac\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "l you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E16QxFTBUrv_",
        "colab_type": "text"
      },
      "source": [
        "# 0. GAN (Generative Adversial Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BHj1QXRxRFq",
        "colab_type": "text"
      },
      "source": [
        "## GAN이란?\n",
        "![대체 텍스트](http://www.codingwoman.com/wp-content/uploads/2018/09/gan-2-700x529.jpg)  \n",
        "- GAN은 가짜 이미지를 생성하는 비지도학습 모델\n",
        "- 텍스트를 받아서 이미지를 생성하는 text-to-iamge synthesis 작업에 사용되기도 함\n",
        "- 이 외에도 모델의 학습을 안정화하는 방향, 이미지로 된 텍스트 같은 여러 종류의 데이터를 활용하는 방향, 더 사실적이고 고화질의 영상을 생성하는 방향 등 다양한 분야로 GAN이 활용되고 있음!\n",
        "> 구분자 학습 부분\n",
        " - 구분자에 가짜 데이터를 넣어주면 구분자의 입장에서는 가짜라고 구분해야 하기에 0 라벨을 사용해서 손실을 계산\n",
        " - 구분자에 진짜 데이터를 넣어주면 구분자의 입장에서는 진짜라고 구분해야 하기에 1 라벨을 사용해서 손실을 계산\n",
        " - 이렇게 계산한 두 손실을 더해서 최종적인 구분자 손실을 구하고 이를 통해 모델을 업데이트\n",
        "\n",
        " > 생성자 학습 부분\n",
        "  - 가짜 데이터를 진짜로 구분되는 것이 생성자의 목적이므로 1 라벨을 이용해 손실을 계산하고 모델을 업데이트\n",
        "\n",
        "- Layer에 Linear 함수 대신 Convolution 연산으로 바꾸고 모델의 layerㄹㄹ 늘리면 이미지 데이터에 대해 더 좋은 결과를 낼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdeSFL6NxROu",
        "colab_type": "text"
      },
      "source": [
        "## 코드 뜯어보기~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Pv7-37oUER",
        "colab_type": "text"
      },
      "source": [
        "### 1. import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CEORYeUoULS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단순한 GAN 모델 생성 및 OrderedDict 사용법\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD2eiQL_oUTl",
        "colab_type": "text"
      },
      "source": [
        "### 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYvvh2kSoaiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 참고\n",
        "# 전치 컨볼루션 연산으로 이미지 크기를 2배로 늘리는 방법 2가지\n",
        "# 둘중에 kernel_size=4,stride=2,padding=1 세팅이 체커보드 아티팩트가 덜합니다.\n",
        "\n",
        "test = torch.ones(1,1,16,16)\n",
        "conv1 = nn.ConvTranspose2d(1,1,kernel_size=4,stride=2,padding=1)\n",
        "out = conv1(test)\n",
        "print(out.size())\n",
        "\n",
        "conv1 = nn.ConvTranspose2d(1,1,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
        "out = conv1(test)\n",
        "print(out.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnxEcKpSoUYf",
        "colab_type": "text"
      },
      "source": [
        "### 2. Hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwAriTt5obPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Hyperparameters\n",
        "# change num_gpu to the number of gpus you want to use\n",
        "\n",
        "epoch = 50\n",
        "batch_size = 512\n",
        "learning_rate = 0.0002\n",
        "num_gpus = 1\n",
        "z_size = 50\n",
        "middle_size = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l55H8LTobWu",
        "colab_type": "text"
      },
      "source": [
        "### 3. Data Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0GRHENoobbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Data\n",
        "\n",
        "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "\n",
        "# Set Data Loader(input pipeline)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2soOkVZ8obf7",
        "colab_type": "text"
      },
      "source": [
        "### 4. Generator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2itRIh2objt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator receives random noise z and create 1x28x28 image\n",
        "# OrderedDict를 사용해 해당 연산의 이름을 지정할 수 있습니다.\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator,self).__init__()\n",
        "        self.layer1 = nn.Sequential(OrderedDict([   # orderdict: 순서가 저장되는 dictionary\n",
        "                        ('fc1',nn.Linear(z_size,middle_size)),\n",
        "                        ('bn1',nn.BatchNorm1d(middle_size)),\n",
        "                        ('act1',nn.ReLU()),\n",
        "        ]))\n",
        "        self.layer2 = nn.Sequential(OrderedDict([\n",
        "                        ('fc2', nn.Linear(middle_size,784)),\n",
        "                        #('bn2', nn.BatchNorm1d(784)),\n",
        "                        ('tanh', nn.Tanh()),\n",
        "        ]))\n",
        "    def forward(self,z):\n",
        "        out = self.layer1(z)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(batch_size,1,28,28)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ric2eXWoboA",
        "colab_type": "text"
      },
      "source": [
        "### 5. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm7sRmJbobsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator receives 1x28x28 image and returns a float number 0~1\n",
        "# we can name each layer using OrderedDict\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator,self).__init__()\n",
        "        self.layer1 = nn.Sequential(OrderedDict([\n",
        "                        ('fc1',nn.Linear(784,middle_size)),\n",
        "                        #('bn1',nn.BatchNorm1d(middle_size)),\n",
        "                        ('act1',nn.LeakyReLU()),  \n",
        "            \n",
        "        ]))\n",
        "        self.layer2 = nn.Sequential(OrderedDict([\n",
        "                        ('fc2', nn.Linear(middle_size,1)),\n",
        "                        ('bn2', nn.BatchNorm1d(1)),\n",
        "                        ('act2', nn.Sigmoid()),\n",
        "        ]))\n",
        "                                    \n",
        "    def forward(self,x):\n",
        "        out = x.view(batch_size, -1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xVMPDJobxN",
        "colab_type": "text"
      },
      "source": [
        "### 6. Put instances on Multi-gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "727e_0kkoUdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put class objects on Multiple GPUs using \n",
        "# torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
        "# device_ids: default all devices / output_device: default device 0 \n",
        "# along with .cuda()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "generator = nn.DataParallel(Generator()).to(device)\n",
        "discriminator = nn.DataParallel(Discriminator()).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPVRL_DGouFw",
        "colab_type": "text"
      },
      "source": [
        "### 7. Check layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlBS6SQIouKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get parameter list by using class.state_dict().keys()\n",
        "\n",
        "gen_params = generator.state_dict().keys()\n",
        "dis_params = discriminator.state_dict().keys()\n",
        "\n",
        "for i in gen_params:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7MxWXi6ouXm",
        "colab_type": "text"
      },
      "source": [
        "### 8. Set Loss function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvYcok0fouep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function, optimizers, and labels for training\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "gen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
        "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
        "\n",
        "ones_label = torch.ones(batch_size,1).to(device)\n",
        "zeros_label = torch.zeros(batch_size,1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVpnFFKFouj6",
        "colab_type": "text"
      },
      "source": [
        "### 9. Restore Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTeRbn8WoucY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model restore if any\n",
        "\n",
        "try:\n",
        "    generator, discriminator = torch.load('./model/vanilla_gan.pkl')\n",
        "    print(\"\\n--------model restored--------\\n\")\n",
        "except:\n",
        "    print(\"\\n--------model not restored--------\\n\")\n",
        "    pass\n",
        "  \n",
        "try:\n",
        "  os.mkdir(\"./model\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"./result\")\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNgMFjEWouVH",
        "colab_type": "text"
      },
      "source": [
        "### 10. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN2DEAl6ouTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "\n",
        "for i in range(epoch):\n",
        "    for j,(image,label) in enumerate(train_loader):\n",
        "        image = image.to(device)\n",
        "        \n",
        "        # 구분자 학습\n",
        "        dis_optim.zero_grad()\n",
        "      \n",
        "        # Fake Data \n",
        "        # 랜덤한 z를 샘플링해줍니다.\n",
        "        z = init.normal_(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
        "        gen_fake = generator.forward(z)\n",
        "        dis_fake = discriminator.forward(gen_fake)\n",
        "        \n",
        "        # Real Data\n",
        "        dis_real = discriminator.forward(image)\n",
        "        \n",
        "        # 두 손실을 더해 최종손실에 대해 기울기 게산을 합니다.\n",
        "        dis_loss = torch.sum(loss_func(dis_fake,zeros_label)) + torch.sum(loss_func(dis_real,ones_label))\n",
        "        dis_loss.backward(retain_graph=True)\n",
        "        dis_optim.step()\n",
        "        \n",
        "        # 생성자 학습\n",
        "        gen_optim.zero_grad()\n",
        "        \n",
        "        # Fake Data\n",
        "        z = init.normal_(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
        "        gen_fake = generator.forward(z)\n",
        "        dis_fake = discriminator.forward(gen_fake)\n",
        "        \n",
        "        gen_loss = torch.sum(loss_func(dis_fake,ones_label)) # fake classified as real\n",
        "        gen_loss.backward()\n",
        "        gen_optim.step()\n",
        "    \n",
        "        # model save\n",
        "        if j % 100 == 0:\n",
        "            print(gen_loss,dis_loss)\n",
        "            torch.save([generator,discriminator],'./model/vanilla_gan.pkl')            \n",
        "            v_utils.save_image(gen_fake.cpu().data[0:25],\"./result/gen_{}_{}.png\".format(i,j), nrow=5)\n",
        "            print(\"{}th epoch gen_loss: {} dis_loss: {}\".format(i,gen_loss.data,dis_loss.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBp5_6rasOTn",
        "colab_type": "text"
      },
      "source": [
        "## DCGAN (Deep Convolutional GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvXsm39jsTyo",
        "colab_type": "text"
      },
      "source": [
        "### DCGAN이란?\n",
        "- GAN 모델이 그냥 결과를 생성하는 것이 아니라 어떤 의미를 가지는 특성 (혹은 표현)을 학습하여 생성할 수 있다는 것을 보여줌\n",
        "> 학습을 잘 시키는 방법\n",
        " - Pooling 연산을 convolution 연산으로 대체하고 Generator network는 Deconvolution 연산을 사용\n",
        " - Generator와 Discriminator에 Batch normalizaton을 사용\n",
        " - Fully Connected network는 사용하지 않기\n",
        " - Generator network는 마지막에 사용되는 tanh 외에는 모든 activation function에 ReLU를 사용\n",
        " - Discriminator network의 모든 activation function로는 Leaky ReLU를 사용\n",
        "- word2vec에서 단어 벡터 간의 연산이 가능하였던 것처럼, DCGAN은 이미지 벡터 간 연산이 가능함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNAPlxk33Twh",
        "colab_type": "text"
      },
      "source": [
        "## SRGAN (Super-resolution GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29KZcCq33XV2",
        "colab_type": "text"
      },
      "source": [
        "### SRGAN이란?\n",
        "- Super-resolution 작업에 GAN을 적용한 모델\n",
        "- SRGAN의 경우 MSE에 추가적으로 생성된 이미지가 진짜 고화질 영상인지 아니면 super-resolution을 거친 이미지인지 구분하는 GAN 손실이 추가로 더해져 고화질 영상의 특성인 선명함이 생성에 영향을 줌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwxiJbPy3XfV",
        "colab_type": "text"
      },
      "source": [
        "## Pix2Pix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghReEmKi422s",
        "colab_type": "text"
      },
      "source": [
        "### Pix2Pix란?\n",
        "- 이미지를 조건으로 주고 이미지를 생성하게 되는 모델\n",
        "- 실제 쌍 데이터가 필요하다는 단점이 있다.\n",
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcv9PqxP5MQz",
        "colab_type": "text"
      },
      "source": [
        "## CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THOMYFgf5Oug",
        "colab_type": "text"
      },
      "source": [
        "### CycleGAN이란?\n",
        "- 꼭 상이 없더라도 Pix2Pix처럼 변환이 가능한 모델"
      ]
    }
  ]
}