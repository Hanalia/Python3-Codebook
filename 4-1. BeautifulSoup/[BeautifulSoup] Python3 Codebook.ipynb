{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(ÏõêÎ≥∏) [BeautifulSoup] Python3 Codebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RYFuejL5sXMt",
        "9jQdCbIKpSe5",
        "hP6PCWjbpS8Q",
        "AwzIWkfVpTAJ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFuejL5sXMt"
      },
      "source": [
        "---\n",
        "# üìÅ Hyun's Code collection (BeautifulSoup) \n",
        "---\n",
        "\n",
        "### <h3 align=\"right\">ü•á Authored by <strong>Hyun</strong></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jQdCbIKpSe5"
      },
      "source": [
        "# ‚úèÔ∏è  What is **BeautifulSoup**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1kDAGBRjU1B"
      },
      "source": [
        "- WebÏóê ÏûàÎäî Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏ÏôÄÏÑú Ï†ÄÏû•Ìï† Ïàò ÏûàÎäî Ìå®ÌÇ§ÏßÄ\n",
        "- ÌÅ¨Î°§ÎßÅÌï† Îïå Í∞ÄÏû• ÎßéÏù¥ Ïì∞Îäî ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
        "- SeleniumÎ≥¥Îã§ ÏóêÎü¨Í∞Ä Ï†ÅÏúºÎ©∞ ÏÜçÎèÑÍ∞Ä Îπ†Î•¥Îã§."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6PCWjbpS8Q"
      },
      "source": [
        "# ‚úèÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRO8gtPDrySn"
      },
      "source": [
        "# Libraries for Crawling\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwzIWkfVpTAJ"
      },
      "source": [
        "# ‚úèÔ∏è  Methods and Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj_sEYeQjTwW"
      },
      "source": [
        "- **requests .get( \" Ï£ºÏÜå \" , headers = {' User-Agent ' : ' Mozilla/5.0 ' } )**: URL Ï£ºÏÜåÏóê Ï†ëÏÜçÏùÑ ÏöîÏ≤≠Ìï¥ Ïó¨Îü¨Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¥\n",
        "  - headerÎ•º Îã¨ÏïÑÏ£ºÎ©¥ anti-crawlingÏùÑ ÌîºÌï¥Í∞à Ïàò ÏûàÏùå\n",
        "  - ÍπêÍπêÌïú ÏÇ¨Ïù¥Ìä∏Îäî user agentÎ•º Ï†ÑÎ∂Ä Îã§ ÏûÖÎ†•Ìï¥Ïïº ÌïòÎäî Í≤ΩÏö∞ÎèÑ ÏûàÏñ¥ÏÑú Í∑∏ÎïåÎäî https://developers.whatismybrowser.com/useragents/explore/software_name/chrome/ Ïó¨Í∏∞Ïóê Îì§Ïñ¥Í∞ÄÏÑú Ï†ÑÏ≤¥Î•º Î≥µÏÇ¨Ìï¥Ïïº Ìï®\n",
        "\n",
        "  **.text**: Ï†ëÏÜçÌïú URLÏùò ÏÜåÏä§ÏΩîÎìú\n",
        "\n",
        "  **.elapsed**: ÌéòÏù¥ÏßÄÍ∞Ä ÏùëÎãµÌïòÎäîÎç∞ Í±∏Î¶∞ ÏãúÍ∞Ñ\n",
        "- **BeautifulSoup ( ÏÜåÏä§ÏΩîÎìú, 'html .parser ' )**: BeautifulSoup Ìï®ÏàòÎäî ÏÑ†ÌÉùÏûêÎ•º ÌÜµÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º ÏÑ†ÌÉùÌï† Ïàò ÏûàÎèÑÎ°ù hyml ÏÜåÏä§ÏΩîÎìúÎ•º ÌÉúÍ∑∏ Í∏∞Ï§ÄÏúºÎ°ú ÌååÏã±Ìï¥Ï§å\n",
        "\n",
        "  **.select ( \" Îç∞Ïù¥ÌÑ∞ ÏúÑÏπò \" )**: ÏÑ†ÌÉùÏûêÎ•º ÏÇ¨Ïö©Ìï¥ÏÑú Îç∞Ïù¥ÌÑ∞Î•º Í≥®ÎùºÏ§å. Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º listÎ°ú Ï†ÄÏû•ÌïúÎã§.\n",
        "  \n",
        "  **.select_one ( \" Îç∞Ïù¥ÌÑ∞ ÏúÑÏπò \" )**: ÏÑ†ÌÉùÏûêÏóê Ìï¥ÎãπÌïòÎäî Îç∞Ïù¥ÌÑ∞ Ï§ë Ï≤´Î≤àÏß∏ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¥\n",
        "  \n",
        "  **.text**: ÏÑ†ÌÉùÌïú Îç∞Ïù¥ÌÑ∞Î•º ÌÖçÏä§Ìä∏ ÌòïÏãùÏúºÎ°ú Î∞îÍøîÏ§å"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aYc9gcmt2hD"
      },
      "source": [
        "### üìî Examples (Crawling Using URL address)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4DyiZK5ZDLo"
      },
      "source": [
        "# Crawl bills contents \n",
        "dict_crawl_data = {}\n",
        "for idx in tqdm(sug.index):\n",
        "    bill_id = sug.loc[idx]['BILL_ID']\n",
        "    url = sug.loc[idx]['DETAIL_LINK']\n",
        "\n",
        "    res = requests.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    \n",
        "    content = soup.select('#summaryContentDiv')[0].text.replace('\\n', '').replace('\\t', '').replace('\\r', '')\n",
        "    dict_crawl_data[bill_id] = content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Qf8JySuxrR"
      },
      "source": [
        "### üìî Examples (Crawling Search Results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu2M1lBcd5tC"
      },
      "source": [
        "# ÏòàÏãú\n",
        "# Îã§Ïùå 'ÏΩîÏïåÎùº' Í≤ÄÏÉâ Í≤∞Í≥º crawling\n",
        "#1.ÏóëÏÖÄ Ï†ÄÏû•\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import openpyxl\n",
        "\n",
        "wb = openpyxl.Workbook()\n",
        "sheet = wb.active\n",
        "sheet.append([\"Ï†úÎ™©\",\"ÎÇ¥Ïö©\"])\n",
        "\n",
        "for p in range(1,3):\n",
        "    raw = requests.get(\"https://search.daum.net/search?w=news&q=ÏΩîÏïåÎùº&p=\"+str(p))\n",
        "    html = BeautifulSoup(raw.text, \"html.parser\")\n",
        "    cons = html.select(\"div.wrap_cont\")\n",
        "\n",
        "\n",
        "    for con in cons:\n",
        "        title = con.select_one(\"a.f_link_b\").text.strip()\n",
        "        ab = con.select_one(\"p.f_eb.desc\").text.strip()\n",
        "        sheet.append([title, ab])\n",
        "\n",
        "wb.save(\"ÏΩîÏïåÎùºnews.xlsx\")\n",
        "\n",
        "#2.CSVÏ†ÄÏû•\n",
        "f = open(\"ÏΩîÏïåÎùºnews.csv\",\"w\", encoding='utf-8')\n",
        "f.write(\"Ï†úÎ™©, ÎÇ¥Ïö©\\n\")\n",
        "\n",
        "for p in range(1,3):\n",
        "    raw = requests.get(\"https://search.daum.net/search?w=news&q=ÏΩîÏïåÎùº&p=\"+str(p)).text\n",
        "    html = BeautifulSoup(raw, \"html.parser\")\n",
        "    cons = html.select(\"div.wrap_cont\")\n",
        "\n",
        "    for con in cons:\n",
        "        title = con.select_one(\"a.f_link_b\").text\n",
        "        ab = con.select_one(\"p.f_eb.desc\").text\n",
        "        #print(title, ab)\n",
        "        title = title.replace(\",\", \"\")\n",
        "        ab = ab.replace(\",\", \"\")\n",
        "        f.write(title+\",\"+ab+\"\\n\")\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kguu10Hku0_U"
      },
      "source": [
        "### üìî Examples (Crawling Daum Movies)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mXW3nbQd5bJ"
      },
      "source": [
        "# ÏòàÏãú\n",
        "# Îã§Ïùå ÏòÅÌôî crawling\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openpyxl\n",
        "\n",
        "try:\n",
        "    # ÏõåÌÅ¨Î∂Å Î∂àÎü¨Ïò§Í∏∞, ÌòÑÏû¨ ÌôúÏÑ±ÌôîÎêú ÏãúÌä∏ ÏÑ†ÌÉùÌïòÍ∏∞\n",
        "    wb = openpyxl.load_workbook(\"week5_hw_1.xlsx\")\n",
        "    sheet = wb.active\n",
        "    print(\"Î∂àÎü¨Ïò§Í∏∞ ÏôÑÎ£å\")\n",
        "# ÌååÏùº Î∂àÎü¨Ïò§Í∏∞Ïóê Ïã§Ìå®ÌïòÎ©¥, ÏÉàÎ°úÏö¥ ÏõåÌÅ¨Î∂Å(ÏóëÏÖÄÌååÏùº)ÏùÑ ÎßåÎì≠ÎãàÎã§.\n",
        "except:\n",
        "    # ÏõåÌÅ¨Î∂Å ÏÉàÎ°ú ÎßåÎì§Í∏∞, ÌòÑÏû¨ ÌôúÏÑ±ÌôîÎêú ÏãúÌä∏ ÏÑ†ÌÉùÌïòÍ∏∞\n",
        "    # Ìó§Îçî Ìñâ Ï∂îÍ∞ÄÌïòÍ∏∞\n",
        "    wb = openpyxl.Workbook()\n",
        "    sheet = wb.active\n",
        "    sheet.append([\"Ï†úÎ™©\", \"ÌèâÏ†ê\", \"Ïû•Î•¥\", \"Í∞êÎèÖ\", \"Î∞∞Ïö∞\"])\n",
        "    print(\"ÏÉàÎ°úÏö¥ ÌååÏùºÏùÑ ÎßåÎì§ÏóàÏäµÎãàÎã§.\")\n",
        "    \n",
        "raw = requests.get(\"http://ticket2.movie.daum.net/Movie/MovieRankList.aspx\",\n",
        "                   headers={'User-Agent':'Mozilla/5.0'})\n",
        "html = BeautifulSoup(raw.text, \"html.parser\")\n",
        "\n",
        "movies = html.select(\"ul.list_boxthumb li\")\n",
        "num=0\n",
        "for movie in movies:\n",
        "    num = num+1\n",
        "    title = movie.select_one(\"div strong a.link_g\")\n",
        "    url = title.attrs[\"href\"]\n",
        "    \n",
        "    raw_each_movie = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "    html_each_movie = BeautifulSoup(raw_each_movie.text, 'html.parser')\n",
        "    \n",
        "    contents = html_each_movie.select('div.detail_summarize')\n",
        "    try:\n",
        "        title = html_each_movie.select_one('div.subject_movie strong.tit_movie')\n",
        "        star = html_each_movie.select_one('em.emph_grade')\n",
        "        genre = html_each_movie.select_one('dl.list_movie > dd:nth-of-type(1)')\n",
        "        director = html_each_movie.select_one('dl.list_movie > dd:nth-of-type(5) a')\n",
        "        actor = html_each_movie.select_one('dl.list_movie > dd:nth-of-type(6) a')\n",
        "        sheet.append([title.text, star.text, genre.text, director.text, actor.text])\n",
        "        print(\"{}Î≤àÏß∏ ÎÅù!\".format(num))\n",
        "    except:\n",
        "        continue\n",
        "wb.save(\"week5_hw_1.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC7qNG1ju-o7"
      },
      "source": [
        "### üìî Examples (Crawling Job site)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSrj9YmPckZe"
      },
      "source": [
        "# ÌåúÎ¶¨Ïø†Î•¥Ìä∏ ÌÅ¨Î°§ÎßÅ ÏΩîÎìú\n",
        "class Crawler:\n",
        "    def __init__(self):\n",
        "        self.name = \"I am a good robot\"\n",
        "\n",
        "    def load_past_data(self, url):\n",
        "        with open(url,'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "    def crawl_data(self):\n",
        "        url = 'http://recruit.dailypharm.com/Search.php?mode=offer&order=reg&optionAreaVal%5B%5D=97&optionAreaVal%5B%5D=100&optionAreaVal%5B%5D=104&optionAreaVal%5B%5D=106&optionAreaVal%5B%5D=107&optionAreaVal%5B%5D=109&optionAreaVal%5B%5D=110&optionAreaVal%5B%5D=111&optionJobVal%5B%5D=17&optionJobVal%5B%5D=12&optionJobVal%5B%5D=13&keyword='\n",
        "        res = requests.get(url)\n",
        "\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "        contents = soup.select(\"li.search_tabCont_wrap a\")\n",
        "\n",
        "        dict_info = {}\n",
        "        for content in contents:\n",
        "            dict_temp = {}\n",
        "            href = content['href']\n",
        "            content_url = 'http://recruit.dailypharm.com' + href\n",
        "            id_ = content_url.split('.php?ID=')[1]\n",
        "            \n",
        "            # crawl title and region\n",
        "            res_2 = requests.get(content_url)\n",
        "            soup_2 = BeautifulSoup(res_2.content.decode('euc-kr','replace'))\n",
        "            title = soup_2.select(\"div.offer_title_wrap h2\")[0].text\n",
        "            region = soup_2.select(\"div.boxList td.boxbody\")[4].text.strip()\n",
        "            \n",
        "            # crawl details\n",
        "            res_3 = requests.get(content_url.replace('OfferView', 'offerContens'))\n",
        "            soup_3 = BeautifulSoup(res_3.content.decode('euc-kr','replace'))\n",
        "            details = ''\n",
        "            for i in soup_3.select(\"body p\"):\n",
        "                details = details + '\\n' + i.text.strip()\n",
        "            \n",
        "            # save crawled informations\n",
        "            dict_temp['title'] = title\n",
        "            dict_temp['region'] = region\n",
        "            dict_temp['details'] = details\n",
        "            dict_info[id_] = dict_temp\n",
        "        return dict_infohtml = BeautifulSoup(raw.text, 'html.parser')\n",
        "print(html)\n",
        "\n",
        "clips = html.select(\"div.inner\")\n",
        "print(clips[0])\n",
        "\n",
        "title = clips[0].select_one(\"dt.title\")\n",
        "print(title)\n",
        "\n",
        "print(title.text)    # ÏÑ†ÌÉùÌïú Îç∞Ïù¥ÌÑ∞Î•º ÌÖçÏä§Ìä∏ ÌòïÏãùÏúºÎ°ú Î∞îÍøîÏ§å"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iffifcZjvFYu"
      },
      "source": [
        "### üìî Examples (Crawling by Pages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f8c9Cy7eIfG"
      },
      "source": [
        "# ÌéòÏù¥ÏßÄ ÎÑòÏñ¥Í∞ÄÍ∏∞\n",
        "for n in range(1, 100, 10):\n",
        "  raw = requests.get(\"https://search.naver.com/search.naver?where=news*query=ÏΩîÏïåÎùº&start=\"+str(n), \n",
        "                  headers={'User-Agent':'Mozilla/5.0'})\n",
        "  html = BeautifulSoup(raw.text, \"html.parser\")\n",
        "  \n",
        "  articles = html.select(\"ul.type01 > li\")\n",
        "  \n",
        "  for article in articles:\n",
        "    title = article.select_one(\"a._sp_each_title\").text\n",
        "    source = article.select_one(\"span._sp_each_source\").text\n",
        "    \n",
        "    print( title, source)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}